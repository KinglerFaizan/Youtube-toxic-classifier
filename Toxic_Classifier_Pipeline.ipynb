{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install --user tensorflow==2.15\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bfe711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faiza\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      comment\n",
      "0                               hi rhys grove\n",
      "1  hi tiff neyfrom next step care hi tiff ney\n",
      "2           i love this song i liked my heart\n",
      "3                                        damn\n",
      "4                                  okay is me\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load DataFrame\n",
    "df = pd.read_csv('youtube_comments_cleaned.csv')  # Replace with your actual dataset file\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cff88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a3daba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before removing labels: Index(['comment'], dtype='object')\n",
      "Columns after removal: Index(['comment'], dtype='object')\n",
      "✅ New labels created based on toxic keywords.\n",
      "                                      comment  label_num\n",
      "0                               hi rhys grove          0\n",
      "1  hi tiff neyfrom next step care hi tiff ney          0\n",
      "2           i love this song i liked my heart          0\n",
      "3                                        damn          1\n",
      "4                                  okay is me          0\n",
      "5                       i miss this boy my jb          0\n",
      "6                        saudades desse tempo          0\n",
      "7                         prime justin bieber          0\n",
      "8            imagine justin bebar in pakistan          0\n",
      "9                        213 my favorite part          0\n",
      "✅ Saved updated dataset with new labels.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('youtube_comments_cleaned.csv')  # Replace with your actual CSV file\n",
    "\n",
    "# Check available columns\n",
    "print(\"Columns before removing labels:\", df.columns)\n",
    "\n",
    "# -----------------------------\n",
    "# Remove existing label columns if any\n",
    "# -----------------------------\n",
    "for col in ['label', 'label_num']:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "        print(f\"✅ Removed existing '{col}' column.\")\n",
    "\n",
    "print(\"Columns after removal:\", df.columns)\n",
    "\n",
    "# -----------------------------\n",
    "# Define toxic keywords\n",
    "# -----------------------------\n",
    "toxic_keywords = [ # English curses\n",
    "    'fuck', 'fucking', 'fucked', 'shit', 'shitty', 'bullshit', 'bitch', 'bitches',\n",
    "    'ass', 'asshole', 'motherfucker', 'mf', 'cunt', 'dick', 'dicks', 'cock',\n",
    "    'pussy', 'faggot', 'fag', 'dyke', 'tranny', 'nigger', 'nigga', 'chink', 'spic', 'kike',\n",
    "    'slut', 'whore', 'bastard', 'retard', 'retarded', 'moron', 'idiot', 'stupid', 'dumb',\n",
    "    'loser', 'worthless', 'pathetic', 'disgusting', 'fat', 'ugly', 'kill yourself', 'kys',\n",
    "    'die', 'drop dead', 'kill', 'burn in hell', 'go to hell', 'hate you', 'villain', 'menace',\n",
    "    'cum', 'cumming', 'jerk off', 'jerk', 'witch', 'demon'\n",
    "    \n",
    "]\n",
    "# Ensure no NaN and all string type\n",
    "df = df.dropna(subset=['comment'])  # Remove rows with NaN comments\n",
    "df['comment'] = df['comment'].astype(str)  # Convert all comments to strings\n",
    "\n",
    "# -----------------------------\n",
    "# Create new labels based on toxic keywords\n",
    "# -----------------------------\n",
    "def assign_label(comment):\n",
    "    comment = str(comment).lower()\n",
    "    for word in toxic_keywords:\n",
    "        if word in comment:\n",
    "            return 1  # toxic\n",
    "    return 0  # non-toxic\n",
    "\n",
    "# Apply labeling function\n",
    "df['label_num'] = df['comment'].apply(assign_label)\n",
    "\n",
    "print(\"✅ New labels created based on toxic keywords.\")\n",
    "print(df[['comment', 'label_num']].head(10))\n",
    "\n",
    "# -----------------------------\n",
    "# Save updated DataFrame with new labels\n",
    "# -----------------------------\n",
    "df.to_csv('youtube_comments_with_new_labels.csv', index=False)\n",
    "print(\"✅ Saved updated dataset with new labels.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ad758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\faiza\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "9/9 [==============================] - 229s 17s/step - loss: 0.5382 - accuracy: 0.8201 - val_loss: 0.1595 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 191s 21s/step - loss: 0.2854 - accuracy: 0.9856 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 162s 16s/step - loss: 0.2055 - accuracy: 0.9856 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 150s 17s/step - loss: 0.1011 - accuracy: 0.9856 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 162s 18s/step - loss: 0.0793 - accuracy: 0.9856 - val_loss: 0.0107 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('toxic_classifier_model\\\\tokenizer_config.json',\n",
       " 'toxic_classifier_model\\\\special_tokens_map.json',\n",
       " 'toxic_classifier_model\\\\vocab.txt',\n",
       " 'toxic_classifier_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Split data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['comment'].tolist(),\n",
    "    df['label_num'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Prepare TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(1000).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights_tensor = tf.constant(class_weights_array, dtype=tf.float32)\n",
    "\n",
    "# Define weighted loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    weights = tf.gather(class_weights_tensor, tf.cast(y_true, tf.int32))\n",
    "    unweighted_loss = loss_object(y_true, y_pred)\n",
    "    weighted_loss = unweighted_loss * weights\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=weighted_loss, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('toxic_classifier_model')\n",
    "tokenizer.save_pretrained('toxic_classifier_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238a981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at toxic_classifier_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at toxic_classifier_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    " # Final hybrid prediction pipeline\n",
    "# -----------------------------\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"toxic_classifier_model\", tokenizer=\"toxic_classifier_model\", framework=\"tf\")\n",
    "\n",
    "def predict_toxicity(comment):\n",
    "    comment_lower = comment.lower()\n",
    "    for word in toxic_keywords:\n",
    "        if word in comment_lower:\n",
    "            return {'label': 'TOXIC', 'score': 1.0}  # keyword override\n",
    "    \n",
    "    # Else use model prediction\n",
    "    result = classifier(comment)[0]\n",
    "    label = 'TOXIC' if result['label'] == 'LABEL_1' else 'NON-TOXIC'\n",
    "    return {'label': label, 'score': result['score']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2978e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'TOXIC', 'score': 1.0}\n",
      "{'label': 'NON-TOXIC', 'score': 0.9782071709632874}\n",
      "{'label': 'TOXIC', 'score': 1.0}\n",
      "{'label': 'TOXIC', 'score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "print(predict_toxicity(\"fuck\"))\n",
    "print(predict_toxicity(\"you are amazing\"))\n",
    "print(predict_toxicity(\"shut up idiot\"))\n",
    "print(predict_toxicity(\"go to hell moron\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab23760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
